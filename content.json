{"posts":[{"title":"","text":"Before we talk about fsync, I want to make sure that everybody knows the ACID principle in database. If you don’t, feel free to look it up over the Internet. So fsync is closely related to the D(durability) od ACID. The principle of durability means that at any given time, the data should be written on disk physically in case of accidents like power outage which cause cause data loss. On POSIX systems, durability is guaranteed through a series of sync operations(fsync(), fdatasync(), aio_fsync()). The fsync() function is intended to force a physical write of data from the buffer cache, and to assure that after a system crash or other failure that all data up to the time of the fsync() call is recorded on the disk. Meanwhile, the fdatasync() function is the same as the fsync() except that it doesn’t update meta-data associated with a file FileBefore we dig deeper into fsync(), we need to know that files in our filesystem consists of two parts, the directory entry and the file itself. For example, if you want to read a file, the filesystem would first fetch the entry for this file, and then open the associated data blob. For those who are familiar with c/c++, you could think of it as the pointer and the data that the pointer points to. Now we know how the file is stored in our filesystem, what about it? Based on fsync()‘s description from here, I quote Calling fsync() does not necessarily ensure that the entry in the directory containing the file has also reached disk. For that an explicit fsync() on a file descriptor for the directory is also needed. Some filesystem like ext3 could do the whole syncing for you, but they all have their own problem based on this article. I would copy and paste them below for better readability. Linux/ext3: If the underlying hard disk has write caching enabled, then the data may not really be on permanent storage when fsync() / fdatasync() return. Linux/ext4: The fsync() implementations in older kernels and lesser used filesystems does not know how to flush disk caches. OSX: For applications that require tighter guarantees about the integrity of their data, Mac OS X provides the F_FULLFSYNC fcntl. The F_FULLFSYNC fcntl asks the drive to flush all buffered data to permanent storage (hey, fsync was supposed to do that, no ? guys ?) (Edit: no, fsync is actually not required to do that – thanks for the clarification Florent!) In conclusion, if you want to ensure durability, you should call fsync() twice, one on the file data and one on its parent directory.","link":"/2023/02/12/About%20fsync/"},{"title":"An interesting bug","text":"Original CodeLet’s take a look at the code snippet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051calculate() { console.log('Call calculate()'); const isCompleted = ...; if (isCompleted) { const answers = ...; this.fetchWhatWeWant(answers); } else { console.log('Before setting latestQuery to null'); this.latestTotalPriceQuery = null; console.log('After setting latestQuery to null'); }}// This fetchWhatWeWant function is already debouncedfetchWhatWeWant(answers) { if (itemPriceAnswers === null) { this.latestQuery = null; return Promise.resolve(); } const endpoint = ...; const payload = ...; const query = postJson(endpoint, payload); query.then((response) =&gt; { if (!_.isEqual(query, this.latestQuery)) { return Promise.resolve(); } console.log('Comparing'); console.log(this.latestQuery); console.log(query); ... return Promise.resolve(); }) .catch(() =&gt; { if (_.isEqual(query, this.latestQuery)) { ... } }) .then(() =&gt; { if (_.isEqual(query, this.latestQuery)) { this.latestQuery = null; } }); console.log('Before setting latestTotalPriceQuery to query'); this.latestTotalPriceQuery = query; console.log('After setting latestTotalPriceQuery to query'); return query;} We thought the code could invalidate the late response that comes from the server, unfortunately, it can’t it still take the last response, which is invalid data, as the valid result. Log Results123456783x Call calculate()Before setting latestTotalPriceQuery to nullAfter setting latestTotalPriceQuery to nullBefore setting latestTotalPriceQuery to queryAfter setting latestTotalPriceQuery to queryComparingPromise { &lt;state&gt;: &quot;fulfilled&quot;, &lt;value&gt;: {…} }Promise { &lt;state&gt;: &quot;fulfilled&quot;, &lt;value&gt;: {…} } This is the log result of the original code, we could see that the reason why the code doesn’t work is because the call chain doesn’t meet our expectations. For example, when we reduce the input from 123 to nothing by hitting the backspace nonstoppingly, the call chain is like 13x calculate() -&gt; setting the latest query to be null -&gt; fetchWhatWeWant() -&gt; postJson() -&gt; return query and set latest query -&gt; recv response In our ideal design, we need the latestQuery to be sth. returned from fetchWhatWeWant(), and the nature of this debounce function forbids us to achieve this goal. In this case, the latestQuery, which aims to buffer the query, makes no effect at all.","link":"/2018/07/02/An%20interesting%20bug/"},{"title":"Client API Design","text":"After building a system, there are several milestones to achieve: Build necessary infra and prove the concept by building a small feature Refine the system by building a larger feature and migrate critical dependent systems Full migration and ongoing doc and wiki","link":"/2017/08/28/Client%20API%20Design/"},{"title":"","text":"","link":"/2023/02/12/About%20redux/"},{"title":"Deadlock(Draft)","text":"Resources Preemptable resource: one that can be taken away from the process owning it with no ill effects, such as memory Non-preemptable resource: one that cannot be taken away from its current owner without potentially causing failure, such as Blu-ray recorder Conditions for Reource Deadlocks Mutual exclusion condition. Each resource is either currently assigned to exactly one process or is available. Hold-and-wait condition. Processes currently holding resources that were granted earlier can request new resources. No-preemption condition. Resources previously granted cannot be forcibly taken away from a process. They must be explicitly released by the process holding them Circular wait condition. There must be a circular list of two or more processes, each of which is waiting for a resource held by the next member of the chain. Recovery from Deadlock Recovery through preemption: take a resource away from its current owner and give it to another process Recovery through rollback: have processes checkpointed periodically Recovery throuth killing processes: kill the current process","link":"/2018/06/26/Deadlock/"},{"title":"Fundamentals on Large-scale Distributed System","text":"Course Notes on COMS W4113 website: https://columbia.github.io/ds1-class/ Challenge 1: large data, many users, more requestsNo single machine can process all data efficiently and we want to enable distribution Challenge 2: scaling is hardAn system may not scale due to multiple reasons and we want to achieve incremental scalability","link":"/2017/09/05/Distributed%20System/"},{"title":"Computational Aspects of Robotics","text":"Course Notes on COMS W4733textbook: Introduction to autonomous mobile robots 2nd editioncourse website: www.cs.columbia.edu/~allen/F17/about.html No midterm LocomotionOwing to limitations, mobile robots generally locomote either using wheeledmechanisms. Key Issues for Locomotion stability characteristics of contact type of environment Legged Mobile RobotsCharacterized by a series of point contacts between the robot and the ground. For a mobile robotwith legs, the total number of distinct event sequences for a walking machine is$$N = (2k - 1)!$$#### Advantages adaptability and maneuverability in rough terrain capable of crossing a hole or chasm so long as its reachexceeds the width of the hole Disadvantages power and mechanical complexity high maneuverability will only be achieved if the legs have a sufficient number of degrees of freedom to impart forces in a number of different directions Wheeled Mobile RobotsAdvantages good efficiency no balance issues Disadvantages traction stability maneuverability Aerial Mobile Robots(I guess I need to post some photos in the future)","link":"/2017/08/28/Computational%20Aspects%20of%20Robotics/"},{"title":"File System Implementation(Draft)","text":"LayoutMost disks can be divided up into one or more partitions with independent file systems on each partition. Implementation of Files Contiguous Allocation Linked-List Allocation Linked-List Allocation Using a Table in Memory I-node(index-node) Implementation of DirectoriesWhen we want to open a file, the OS uses the path name provided by users to locate the directory entry on the disk, which provides the information to find the disk blocks. In general, directories are served to map the ASCII name of the file onto some info which is necessary for finding the data. Shared FilesLet’s assume A has a file to share with B. If directories of B contains disk addresses, then a copy of the addresses will have to be made in B’s directory. If the file is appended, the new blocks appended will be listed only in the directory of the user doing the append. There are two solutions to this problem: one is callled i-node(UNIX) or symbolic link. Log Structure File Systems Structure the whole disk as a log Since i-nodes’ address can not simply be calculated, an i-node map is maintained. All writes are initially buffered inmemory, and periodically all the buffered writes are written to the disk in a single segment, at the end of the log. A cleaner thread that spends its time scanning the log circularly to compact it. Like a circular buffer Measurements given in the papers cited above show that LFS outperforms UNIX by an order of magnitude on small writes, while having a performance that is as good as or better than UNIX for reads and large writes. Journaling File SystemsThe basic idea here is to keep a log of what the file system is going to do before it does it, so that if the system crashes before it can do its planned work, upon rebooting the system can look in the log to see what was going on at the time of the crash and finish the job. Virtual File SystemThe virtual file system(VFS) has an upper interface to user processes and it is the well-known POSIX interface. Then comes the VFS interface","link":"/2018/06/20/File%20System%20Implementation/"},{"title":"Elastic Distributed Database","text":"This project aims to develop an elastic distributed database using MySQL replication to guarantee high availability and dynamic scalability and use it to serve as the backend for a multi-tier web application running TPC-W benchmark.## Configuration on single ec2 Config on AWS and install necessary tools on ec2(master node). password for mysql is : TigerBit!2016(default) 1234sudo apt-get updatesudo apt-get install -y openjdk-7-jdk ant gcc links git makesudo apt-get install mysql-serversudo apt-get install tomcat7 on slave nodes and candidate node, only mysql and java needs to be installed. Clone repository 12git clone https://github.com/jopereira/java-tpcw.gitgit clone https://github.com/peterbittiger/elasticDB.git Initialize Database 1mysql -uroot -p &lt; /home/ubuntu/elasticDB/tpcw/mysql.sql Deployment 1234567cd java-tpcwpatch -p1 &lt; /home/ubuntu/elasticDB/tpcw/tpcw.patch // apply patchant dist sudo rm -rf /var/lib/tomcat7/webapps/tpcw* // not necessary for the first timesudo cp /home/ubuntu/java-tpcw/dist/tpcw.war /var/lib/tomcat7/webapps ant gendb // populate databasesudo ant genimg // presentation tier Log in the web page 1http://&lt;ip address&gt;:8080/tpcw/TPCW_home_interaction Configuration on multiple ec2Use our own laptop as client emulator, which is respond for sending different request. Set root password to: 0307 12sudo supasswd Change config to allow access from outside 1vi /etc/ssh/sshd_config change $PermitRootLogin$ to yes and $PasswordAuthentication$ to yes and then restart ssh service. 1service ssh restart Copy key-pairs between machines 1234sudo sucd ~ssh-keygen -t rsassh-copy-id root@&lt;ip address&gt; my computer copy to 4 ec2 instances, master copy to 4 ec2 instances(including itself), slaves should copy to the other 3 ec2 instances. Copying key of $a$ to $c$ means that $c$ grants access to $a$ Clone repo 1git clone https://github.com/peterbittiger/elasticDB.git change the ip address in set_env.sh and run testConnection.sh to test communications MySQL Replication TestRun prepareMasterSlaves.sh and then log in to master node and run mysql 12mysql -uroot -p#pwd: TigerBit!2016 inside mysql enter the following commands: 12mysql&gt; use canvasjs_db;mysql&gt; select * from datapoints; download related dependency for this project: 12mvn dependency:resolvemvn eclipse:eclipse and then change ipaddress in tpcw.properties in our project: 123writeQueue = &lt;MASTER_IP&gt;readQueue = &lt;MASTER_IP&gt;,&lt;SLAVE1_IP&gt;,&lt;SLAVE2_IP&gt;candidateQueur = &lt;CANDIDATE_IP&gt; Before we run the project, we must re-run the prepareMasterSlaves.sh, which would take approximately 5 minutes. Next, run enableMonitors.sh, and we will see new tabs showing performance.","link":"/2017/08/28/Elastic%20Distributed%20Database/"},{"title":"File Systems(Draft)","text":"FilesInodeA data structure in a Unix-style file system which describes a filesystem object such as a file or a directory.(basic unit of meta data) File System TypesThere are several kinds of file systems, older versions of systems use FAT,while newer systems version like Windows 8 use NTFS File StructureThere are three common structures for a file: byte sequence, record sequence and tree File Types Directories are actually system files for maintaining the structure of the whole file system. Regular files contain user information and are generally either ASCII files or binary files. Normally, a binary file from early UNIX system consists of 5 sections: header, text, data, relocationi bits and symbol table. The header will start with a so-called magic number, which would identify the file as an executable file. Then comes the sizes of various pieces of the file and etc. Another example is archives, which I don’t know much about it. File Attributesaka metadata, password is also one of the attributes. File Descriptorssmall integers returned when a file is opened. Directoriesdirectories are also known as folders and they are used to keep track of files. Absolute PathThe first character of the path name is the separator(in UNIX, it is ‘/‘) Relative PathUsed in conjunction with the concept of the working directory LinkHard LinkCan’t cross disk boundaries but efficient. A technique that allows a file to appear in more than one directory. It would increment the counter in the file’s i-node (to keep track of the number of directory entries containing thefile) Symbolic LinkCan cross disk boundaries but less efficient. Instead of having two names point to the same internal data structure representing a file, a name can be created that points to a tiny file naming another file","link":"/2018/06/18/File%20Systems/"},{"title":"Global Interpreter Lock(GIL)","text":"Python is a very handy tool in terms of web development, machine learning and many other areas; however, it is also well-known for its poor performance in parallell computing due to global interpreter lock(GIL), which only exists in Cython. As I see it, its use of GIL doesn’t bother me a lot, but it is still very intriguing and worth some time to investigate why lies beneath the GIL. I came across this slide on the train to Cupertino. Performance ExperienceLet’s consider a simple function 123def count(n): while n &gt; 0: n -= 1 compare the result of running it with and without threads. It turns out that the one with threads are far more slower than the other one. The result is based on a multi-core CPU and it you disable one of the CPU cores, the performance for the threaded program becomes slightly better. Thread CreationWhen you create a thread, python would create a small data structure containing some interpreter state and fires off a thread and call the function PyEval_CallObject. Thread ExecutionThe interpreter has a global variable that points to the thread state structure of the current thread. So in conclusion, only one thread can execute in the interpreter at a time. GIL BehaviorThreads hold the GIL when running; however, they would relaese it when blocked for I/O. When it deals with CPU-bound threads like our function, the interpreter periodically performs a “check” every 100 interpreter “ticks” by default. So what happens during the periodic check? There are many two things: In the main thread only, signal handlers will execute if there are any pending signals (more shortly) Release and reacquire the GIL123456789101112131415161718192021// ceval.cif (--_Py_Ticker &lt; 0) { if (*next_instr == SETUP_FINALLY) { /* Make the last opcode before a try: finally: block uninterruptible. */ goto fast_next_opcode; } _Py_Ticker = _Py_CheckInterval; tstate-&gt;tick_counter++; ....... /*for pending*/}if (interpreter_lock) { /* Give another thread a chance */ if (PyThreadState_Swap(NULL) != tstate) Py_FatalError(&quot;ceval: tstate mix-up&quot;); PyThread_release_lock(interpreter_lock); /* Other threads may run now */ PyThread_acquire_lock(interpreter_lock, 1);} If a signal arrives, the interpreter runs the “check” after every tick until the main thread runs. Since signal handlers can only run in the main thread, the interpreter quickly acquires/releases the GIL after every tick until it gets scheduled, which is why it is that slow in threaded program. This is partly why signals get so weird (the interpreter has no control over scheduling so it just attempts to thread switch as fast as possible with the hope that main will run). Priority Inversion in Multicore ProcessorA CPU-bound thread (low priority) can be blocking the execution of an I/O-bound thread (high priority). It occurs because the I/O thread can’t wake up fast enough to acquire the GIL before the CPU-bound thread reacquires it.","link":"/2018/05/26/GIL/"},{"title":"Input and Output(Draft)","text":"I/O Devices Block devices: stores information in fixed-size blocks, each one with its own address. Character devices: delivers or accepts a stream of characters, without regard to any block structure. I/O units often consist of a mechanical component and an electronic component. The electronic component is called the device controller or adapter. The mechanical component is the device itself. Normally the controller would receive a serial bit stream starting with a preamble, then the 4096 bits in a sector, and finally a checksum, or ECC (Error-Correcting Code). Its job is to convert the serial bit stream into a block of bytes and perform any error correction necessary. Communication between CPU and Control Register Assign control register an I/O port number Memory-mapped I/O, which is to map all control registers into memory space by assigning a unique memory address. With it, the I/O device driver can be written entirely in C and no special protection mechanism is needed to keep user processes from performing I/O. However, a device control register should not be cached.","link":"/2018/06/26/Input%20and%20Output/"},{"title":"Install Caffe on Ubuntu 14","text":"Assuming you have already installed CUDA and cudnn as well as anaconda## Install OpenCV 1234567891011121314151617181920sudo apt-get updatesudo apt-get install -y build-essentialsudo apt-get install -y cmakesudo apt-get install -y libgtk2.0-devsudo apt-get install -y pkg-configsudo apt-get install -y python-numpy python-devsudo apt-get install -y libavcodec-dev libavformat-dev libswscale-devsudo apt-get install -y libjpeg-dev libpng-dev libtiff-dev libjasper-dev sudo apt-get -qq install libopencv-dev build-essential checkinstall cmake pkg-config yasm libjpeg-dev libjasper-dev libavcodec-dev libavformat-dev libswscale-dev libdc1394-22-dev libxine-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev libv4l-dev python-dev python-numpy libtbb-dev libqt4-dev libgtk2.0-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev libvorbis-dev libxvidcore-dev x264 v4l-utilswget http://downloads.sourceforge.net/project/opencvlibrary/opencv-unix/2.4.11/opencv-2.4.11.zipunzip opencv-2.4.11.zipcd opencv-2.4.11mkdir releasecd releasecmake -G &quot;Unix Makefiles&quot; -D CMAKE_CXX_COMPILER=/usr/bin/g++ CMAKE_C_COMPILER=/usr/bin/gcc -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_OPENGL=ON -D BUILD_FAT_JAVA_LIB=ON -D INSTALL_TO_MANGLED_PATHS=ON -D INSTALL_CREATE_DISTRIB=ON -D INSTALL_TESTS=ON -D ENABLE_FAST_MATH=ON -D WITH_IMAGEIO=ON -D BUILD_SHARED_LIBS=OFF -D WITH_GSTREAMER=ON -D CUDA_GENERATOR=Kepler ..make all -j8sudo make install After installation, run 1sudo gedit /etc/ld.so.conf.d/opencv.conf and add 1/usr/local/lib in the file and afterwards run 12sudo ldconfigsudo gedit /etc/bash.bashrc and add 12PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfigexport PKG_CONFIG_PATH Install ffmpeg(Optional)1234sudo add-apt-repository ppa:mc3man/trusty-mediasudo apt-get updatesudo apt-get dist-upgradesudo apt-get install ffmpeg Install CaffeModify Makefile.config 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120## Refer to http://caffe.berkeleyvision.org/installation.html# Contributions simplifying and improving our build system are welcome!# cuDNN acceleration switch (uncomment to build with cuDNN).USE_CUDNN := 1# CPU-only switch (uncomment to build without GPU support).# CPU_ONLY := 1# uncomment to disable IO dependencies and corresponding data layers# USE_OPENCV := 0# USE_LEVELDB := 0# USE_LMDB := 0# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)# You should not set this flag if you will be reading LMDBs with any# possibility of simultaneous read and write# ALLOW_LMDB_NOLOCK := 1# Uncomment if you're using OpenCV 3# OPENCV_VERSION := 3# To customize your choice of compiler, uncomment and set the following.# N.B. the default for Linux is g++ and the default for OSX is clang++# CUSTOM_CXX := g++# CUDA directory contains bin/ and lib/ directories that we need.CUDA_DIR := /usr/local/cuda# On Ubuntu 14.04, if cuda tools are installed via# &quot;sudo apt-get install nvidia-cuda-toolkit&quot; then use this instead:# CUDA_DIR := /usr# CUDA architecture setting: going with all of them.# For CUDA &lt; 6.0, comment the *_50 through *_61 lines for compatibility.# For CUDA &lt; 8.0, comment the *_60 and *_61 lines for compatibility.CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \\ -gencode arch=compute_20,code=sm_21 \\ -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_52,code=sm_52 \\ -gencode arch=compute_60,code=sm_60 \\ -gencode arch=compute_61,code=sm_61 \\ -gencode arch=compute_61,code=compute_61# BLAS choice:# atlas for ATLAS (default)# mkl for MKL# open for OpenBlasBLAS := atlas# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.# Leave commented to accept the defaults for your choice of BLAS# (which should work)!# BLAS_INCLUDE := /path/to/your/blas# BLAS_LIB := /path/to/your/blas# Homebrew puts openblas in a directory that is not on the standard search path# BLAS_INCLUDE := $(shell brew --prefix openblas)/include# BLAS_LIB := $(shell brew --prefix openblas)/lib# This is required only if you will compile the matlab interface.# MATLAB directory should contain the mex binary in /bin.# MATLAB_DIR := /usr/local# MATLAB_DIR := /Applications/MATLAB_R2012b.app# NOTE: this is required only if you will compile the python interface.# We need to be able to find Python.h and numpy/arrayobject.h.# PYTHON_INCLUDE := /usr/include/python2.7 \\# /usr/lib/python2.7/dist-packages/numpy/core/include# Anaconda Python distribution is quite popular. Include path:# Verify anaconda location, sometimes it's in root.ANACONDA_HOME := $(HOME)/anacondaPYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ $(ANACONDA_HOME)/include/python2.7 \\ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include# Uncomment to use Python 3 (default is Python 2)# PYTHON_LIBRARIES := boost_python3 python3.5m# PYTHON_INCLUDE := /usr/include/python3.5m \\# /usr/lib/python3.5/dist-packages/numpy/core/include# We need to be able to find libpythonX.X.so or .dylib.# PYTHON_LIB := /usr/libPYTHON_LIB := $(ANACONDA_HOME)/lib# Homebrew installs numpy in a non standard path (keg only)# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include# PYTHON_LIB += $(shell brew --prefix numpy)/lib# Uncomment to support layers written in Python (will link against Python libs)# WITH_PYTHON_LAYER := 1# Whatever else you find you need goes here.INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/includeLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies# INCLUDE_DIRS += $(shell brew --prefix)/include# LIBRARY_DIRS += $(shell brew --prefix)/lib# NCCL acceleration switch (uncomment to build with NCCL)# https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)# USE_NCCL := 1# Uncomment to use `pkg-config` to specify OpenCV library paths.# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)USE_PKG_CONFIG := 1# N.B. both build and distribute dirs are cleared on `make clean`BUILD_DIR := buildDISTRIBUTE_DIR := distribute# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171# DEBUG := 1# The ID of the GPU that 'make runtest' will use to run unit tests.TEST_GPUID := 0# enable pretty build (comment to see full commands)Q ?= @ and then run 1234make all -j8make test -j8make runtestmake pycaffe","link":"/2017/08/28/Install%20Caffe%20on%20Ubuntu%2014/"},{"title":"Intellisense in IDE","text":"OverviewDuring my internship this summer, our team wants to build a cloud IDE for other developers in the company and I was in charge of the autocompletion feature. It was absolutely a challenge for cause I have done anything related to it at all. So it took me a while to know how the whole system works. Due to security concern, I am not allowed to show you the code. However, this blog can still serve as an introduction and a basic tutorial for those who wants to equip their editor with intellisense. Before I proceed to building our own cloud IDE, I did some researches on how IDE liek pycharm or eclipse provide intellisense; and I discovered that most of them make use of the compiler APIs, which provide rich information about the code snippet. In addition, with a more thorough dive into VS Code, I found out a handy tool called Microsoft Language Server Portocol. As I see it, instead of re-building our own interface to different compilers, why not make use of all kinds of existing language servers, which can save us the cost of maintainance. Accordingly, we decide to use Monaco as our editor, which is a core component of VS Code, and luckily, now all I have to do is to build a language client that can communicate with languages servers with jsonrpc. To Be Continuedblah blah blah","link":"/2017/08/31/Intellisense%20in%20IDE/"},{"title":"Kademlia(Draft)","text":"IntroductionKademlia is the name of a peer-to-peer distributed hash table(DHT). The reason why I write this blog is because I’m currently writing a bittorrent client in golang and using the DHT to discover peers is the next step of the client implementation. Here is a tutorial writen in chinese, which is quite straightforward compared with the paper. System DescriptionLike other DHTs(I know nothing about them LOL). a 160-bit opaque ID is assigned to each node and XOR is applied in calculating distance between nodes. A binary tree representation is used to represent the whole space, which can speed up the lookup process of any given node with constant time. Each node know at least one node in each subtree and the subtree is determined using a prefix. The highest subtree consist of the half of the binary tree not containing the node, then the next subtree consists of the remaining subtree not containing the node, and so forth. K-BucketEach node has a list of k-buckets, and each k-bucket stores &lt;IP address, UDP port, Node ID&gt;s(the triples represent a node) of distance between $2^i$ and $2^{i+1}$ from itself. Each bucket is ordered by placing the most-recently seen node to the tail of the bucket. k, as a parameter, represents the bucket size, not the list size. The list size is the number of buckets, in this paper, it should be 160 cause the 160-bit node representation. Kademlia ProtocolThe protocol consists of four RPCs: PING, STORE, FIND_NODE, and FIND_VALUE. PING: probes a node to see if it is online STORE: instructs a node to store a &lt;key,value&gt; pair for later retrieval FIND_NODE: takes a 160-bit ID as an arguent and returns k closest node with &lt;IP address, UDP port, Node ID&gt; FIND_VALUE: no idea what it does Routing TableThe routing table is a binary tree of which the subtrees are k-buckets.","link":"/2018/07/24/Kademlia/"},{"title":"Javascript Core(Draft)","text":"This article is based on Javascript Core. Object and PrototypeAlthough js is known as “object-oriented” programming language, it is quite different from my understanding of OOP. Mostly because they didn’t actually teach OOP in college :D.First concept to take in mind is: An object is a collection of properties, and has single prototype object. The prototype may be either an object or the null value. Every object when created, received its prototype, which is a delegation object used to implement prototypbe-based inheritance. Based on the previous experience with C++, I think of it as the base class at first. In other words, for a prototype chain:A-&gt;B-&gt;Object.prototype-&gt;null. A is inherited from B and A has a reference to it, which is the dunder proto(proto), and so on. When a property is not found in the object, we’ll attempte to resolve it in the prototype. This mechanism is known as dynamic dispatch or delegation. ClassThe most astonishing part of that article comes next, which change my understanding of class. In js, class is more like a syntax sugar, consisting of a constructor function and prototype. The prototype is stored in the constructor function’s prototype property. Creating a prototype and sets the prototype for newly created objects is what class constructor does. You could go to that article and see the corresponding code, which will definitely enlighten you. Execution ContextHonestly, I still don’t know how to describe what context is. But there is something that I learnt from this article. The yield statement returns the value to the caller and pops the context while the next statement push the context onto stack and resume function. EnvironmentSo every execution context has an associated lexical environment, which is a structure used to define association between identifiers in the context. In that case, an environment has variables, functions and classes defined in it. We could think of it as a pair, consisting of an environment record (an actual storage table which maps identifiers to values), and a reference to the parent (which can be null). The keyword var and with would provide the object as binding objects(new concept, never heard of it LOL). The binding object of the global environment is the global object and equals to this. Binding object can store a name which is not added to the environment record, since it’s not a valid identifier: 1this['not valid ID'] = 30; FunctionFirst take a look at two definitions from the article: First-class function: a function which can participate as a normal data: be stored in a variable, passed as an argument, or returned as a value from another function. Free variable: a variable which is neither a parameter, nor a local variable of this function. For the following code, the x is a free variable and the program would resolve its binding from the outer scope where the function was created instead of from the caller scope, from where the function is called(dynamic scoping). That’s why it is 10 intstead of 20. To implement lexical scoping, we need to capture the environment where the function is created, so think of it as every function has a reference to the environment. ClosureIf you took a loot at the last section, you can understand closure better Closure: A closure is a function which captures the environment where it’s defined. Further this environment is used for identifier resolution. ThisUnlike other variables that are static scoped, this value is dynamically scoped and it can not mutate. The purpose of it is to executed the same code for multiple objects(think about clss-based OOP). For example, we are unable to determine this value by looking at the following code: 123function foo() { return this;} But there is an exception, the arrow functions are special in terms of this value: their this is lexical (static). Holy shit, I can’t understand the example code from the article RealmI have never heard of realm before, based on this article, before code is evalutated, all code must be associated with a realm, which provides a global environment for a context. Realm: A code realm is an object which encapsulates a separate global environment. Job Job: A job is an abstract operation that initiates an ECMAScript computation when no other ECMAScript computation is currently in progress. Jobs are enqueued on the job queues, and in current spec version there are two job queues: ScriptJobs, and PromiseJobs. And initial job on the ScriptJobs queue is the main entry point to our program — initial script which is loaded and evaluated: a realm is created, a global context is created and is associated with this realm, it’s pushed onto the stack, and the global code is executed. Agent Agent: An agent is an abstraction encapsulating execution context stack, set of job queues, and code realms. The agents are state isolated from each other, and can communicate by sending messages. Some data can be shared though between agents, for example SharedArrayBuffers. Agents can also combine into agent clusters.","link":"/2018/07/02/Javascript%20Core/"},{"title":"Introduction to Kafka","text":"About KafkaOverviewKafka is a distributed messaging system for collecting and delivering high volumes of log data with low latency. On the one hand, Kafka is distributed and scalable, and offers high throughput. On the other hand, Kafka provides an API similar to a messaging system and allows applications to consume log events in real time#### Traditional Messaging SystemTraditional enterprise messaging systems are not to be a good fit for log processing. The reason is as follows:1. There is a mismatch in features offered by enterprise systems like out of order and unneeded features2. Many systems do not focus as strongly on throughput as their primary design constraint like no batch operations3. Weak in distributed support4. Many messaging systems assume near immediate consumption of messages, so the queue of unconsumed messages is always fairly small.#### Kafka A stream of messages of a particular type is defined by a topic. To balance load, a topic is divided into multiple partitions and each broker stores one or more of those partitions. A producer can publish messages to a topic. The published messages are then stored at a set of servers called brokers. A consumer can subscribe to one or more topics from the brokers, and consume the subscribed messages by pulling data from the brokers ##### 1. Efficiency on Single Partition- Simple Storage:Every time a producer publishes a message to a partition, the broker simply appends the message to the last segment file. The segment files are written to disk only after a configurable number of messages have been published or a certain amount of time has elapsed. A message is only exposed to the consumers after it is flushed and a message stored in Kafka does not have an explicit message id. Instead, each message is addressed by its logical offset in the log.- Efficient Transfer:Avoid explicitly caching messages in memory at the Kafka layer. Instead, cache them on the underlying file system page cache, which avoids double buffering. Since Kafka does not cache messages in process at all, it has very little overhead in garbage collecting its memory, making efficient implementation in a VM-based language feasible. Plus, Kafka is a multi-subscriber system and a single message may be consumed multiple times by different consumer applications- Stateless Broker:The volume of info consumed is stored in consumer, which reduce complexity and the overhead on the broker. A message is automatically deleted if it has been retained in the broker longer than a certain period, typically 7 days; thus, A consumer can deliberately rewind back to an old offset and re-consume data.##### 2. Distributed Coordination- A partition within a topic is the smallest unit of parallelism. Kafka has the concept of consumer groups. Each consumer group consists of one or more consumers that jointly consume a set of subscribed topics, and each message is delivered to only one of the consumers within the group.- Not having a central “master” node, but instead let consumers coordinate among themselves in a decentralized fashion. To facilitate the coordination, we employ a highly available consensus service Zookeeper. Zookeeper has a very simple, file system like API and it has the following functions: Zookeeper can create a path, set the value of a path, read the value of a path, delete a path, and list the children of a path. It does a few more interesting things: (a) one can register a watcher on a path and get notified when the children of a path or the value of a path has changed; (b) a path can be created as ephemeral (as oppose to persistent), which means that if the creating client is gone, the path is automatically removed by the Zookeeper server; (c) zookeeper replicates its data to multiple servers, which makes the data highly reliable and available. Kafka uses Zookeeper for the following tasks: (1) detecting the addition and the removal of brokers and consumers, (2) triggering a rebalance process in each consumer when the above events happen, and (3) maintaining the consumption relationship and keeping track of the consumed offset of each partition##### 3. Delivery Guarantees- In general, Kafka only guarantees at-least-once delivery- Kafka guarantees that messages from a single partition are delivered to a consumer in order Install KafkaDownload and extract Kafka from http://kafka.apache.org/downloads.html. For me, I have unzipped it in C:\\Kafka## Set Up Kafka Go to your Kafka config directory. For me its C:\\Kafka\\kafka_2.11-0.10.1.0\\config Edit file “server.properties” Find &amp; edit line “log.dirs=/tmp/kafka-logs” to “log.dir= C:\\Kafka\\kafka_2.11-0.10.1.0\\kafka-logs”.(Actually it doesn’t matter where you put your log) If your Zookeeper is running on some other machine or cluster you can edit “zookeeper.connect:2181” to your custom IP and port. For this demo we are using same machine so no need to change. Also Kafka port &amp; broker.id are configurable in this file. Leave other settings as it is. Your Kafka will run on default port 9092 &amp; connect to zookeeper’s default port which is 2181. Run Kafka Server Go to your Kafka installation directory C:\\Kafka\\kafka_2.11-0.10.1.0\\ Open a command prompt here by pressing Shift + right click and choose “Open command window here” option) Now type .\\bin\\windows\\kafka-server-start.bat .\\config\\server.properties and press Enter.(Before you run Kafka server, make sure you have opened ZooKeeper)","link":"/2017/08/28/Kafka/"},{"title":"Kalman Filter II(Chinese)","text":"（二）Kalman Filter公式推导考虑一个用状态空间模型描述的动态系统$$X(t+1)=\\Phi X(t) + \\Gamma W(t) $$ $$Y(t)=H X(t) + V(t)$$其中，$t$ 为时间，系统在时刻 $t$ 的状态为 $X(t) \\in R^n$ ；Y(t)为观测信号； $W(t) \\in R^r$ 为输入的噪声， $V(t)\\in R^m$ 为观测噪声。由于不同问题的状态空间描述是不一样的，因此只有在确定状态空间后才可以得到递推公式（这是我自己的看法，不知道对不对，可能递推公式还是一样，不过根据问题来推一遍公式总不会有错的，只是麻烦点）。 【假设1】 $W(t)$ 和 $V(t)$ 是均值为零，方差各为 $Q$ 和 $R$ 的不相关白噪声 【假设2】 初始状态 $X(0)$ 与 $W(t)$ 和 $V(t)$ 不相关，而且$$E[X(0)] = \\mu_0,E[(X(0)-\\mu_0)(X(0)-\\mu_0)^T] = P_0$$ 推导 问题重述 卡尔曼滤波是一个基于观测信号 ${Y(1),Y(2),…,Y(t) }$ ，求状态 $X(k)$ 的线性最小方差估计值的问题 $\\hat X(k|t)$ ，而且这个问题的目标函数是$$J=E[(X(k)-\\hat X(k|t))^T(X(k)-\\hat X(k|t))]$$那么由之前的定义1.1可以得到，卡尔曼滤波问题可以归为求射影$$\\hat X(k|t) = proj(X(k)|Y(1),Y(2),…,Y(t))$$由定义1.5可以得到递推公式为$$\\hat X(t+1|t+1) = \\hat X(t+1|t)+K(t+1)\\epsilon (t+1)$$$$K(t+1)=E[X(t+1)\\epsilon^T(t+1)]{ E[\\epsilon (t+1)\\epsilon^T(t+1)]}^{-1}$$其中$K(t+1)$ 为卡尔曼滤波器增益。 对状态方程求射影 以 $w$ 表示观测信号，对状态方程 $X(t+1)=\\Phi X(t) + \\Gamma W(t) $ 两边取射影有（射影运算具有分配律，跟向量点乘一样）$$proj(X(t+1)|w) = \\Phi proj( X(t)|w)+\\Gamma proj(W(t)|w)$$$$\\hat X(t+1|t) = \\Phi \\hat X(t|t)+\\Gamma proj(W(t)|w)$$迭代求解状态方程后获得原状态方程的另一种表示$$X(t+1) = \\Phi^{t+1}X(0) + \\Gamma [\\Phi^0 W(t)+\\Phi^1 W(t-1)+…+\\Phi^t W(0)]$$对于没有控制量的状态方程，由于假设1的白噪声的性质和假设2，我们可以知道 $X(0),W(0),…,W(t)$ 互不相关，即正交，因此可以把它们看作基，而 $X(t+1)$ 是可以由它们表示，所以有$$X(t+1) \\in L(W(t),…,W(0),X(0))$$当然，如果状态方程里面有控制量$U(t)$的加入，即$$X(t+1)=\\Phi X(t) + \\Psi U(t) + \\Gamma W(t) $$相应递推公式为$$X(t+1) = \\Phi^{t+1}X(0) +[\\Phi^0 \\Psi U(t)+\\Phi^1 \\Psi U(t-1) +…+\\Phi^t\\Psi U(0)]+ [\\Phi^0 \\Gamma W(t)+\\Phi^1\\Gamma W(t-1)+…+\\Phi^t\\Gamma W(0)] $$而有控制量的状态方程，我们认为每个时刻的控制量都是与公式中其它变量不相关，所以同理有$$X(t+1) \\in L(U(t),…,U(0),W(t),…,W(0),X(0))$$ 对观测方程求射影 考虑上文中不带控制量的状态方程的推导，同理，对观测方程做同样的运算有$$Y(t+1) \\in L(V(t+1),W(t)…,W(0),X(0))$$所以有$$L(Y(1),Y(2),…,Y(t)) \\subset L(V(t),…,V(1),W(t-1),…,W(0),X(0))$$ 状态方程和观测方程的递推式 由于假设，可以知道 $W(t)$ 与 $ {V(t),…,V(1),W(t-1),…,W(0),X(0)}$不相关（正交），所以有$$W(t) \\bot L(Y(1),Y(2),…,Y(t))$$因为正交，所以投影后的状态方程尾项为零，即$$\\hat X(t+1|t) = \\Phi \\hat X(t|t)$$对观测方程做同样的推导可以同理得到$$V(t+1) \\bot L(Y(1),Y(2),…,Y(t))$$$$\\hat Y(t+1|t) =H \\hat X(t+1|t) $$ 协方差更新公式推导 将滤波器的预报估计值误差和协方差记为$$\\widetilde X(t|t) = X(t)-\\hat X(t|t)$$ $$\\widetilde X(t+1|t)=X(t+1)-\\hat X(t+1|t)$$ $$P(t|t) = E[\\widetilde X(t|t) \\widetilde X^T(t|t)]$$ $$P(t+1|t) = E[\\widetilde X(t+1|t)] \\widetilde X^T(t+1|t)]$$由之前的定义1.3，引入新息（不需要管这个名词怎么来的）的表达式$$\\epsilon(t+1) = Y(t+1)-proj(Y(t+1)|Y(1),Y(2),…,Y(t))=Y(t+1)-\\hat Y(t+1|t)=[H X(t+1)+V(t+1)]-[H\\hat X(t+1|t)] = H \\widetilde X(t+1|t)+V(t+1)$$下面进行估计值的误差和协方差的推导$$\\widetilde X(t+1|t) = X(t+1)-\\hat X(t+1|t)$$$$= [\\Phi X(t)+\\Gamma W(t)]-[\\Phi \\hat X(t|t)]$$$$= \\Phi \\widetilde X(t|t) +\\Gamma W(t)$$ 另外一个公式 $$\\widetilde X(t+1|t+1) = X(t+1)-\\hat X(t+1|t+1)$$$$= X(t+1) - [\\hat X(t+1|t)+K(t+1)\\epsilon (t+1)] $$$$= \\widetilde X(t+1|t) - K(t+1)\\epsilon(t+1) $$$$= \\widetilde X(t+1|t) - K(t+1)[H\\widetilde X(t+1|t)+V(t+1)]$$$$= [I-K(t+1)H]\\widetilde X(t+1|t) - K(t+1)V(t+1)$$在计算协方差更新公式时就要利用之前进行射影运算后得到的正交关系。由于$X(t) \\in L(W(t-1),…,W(0),X(0))$，而且 $\\hat X(t|t)$ 是 $X(t)$ 在流型上的投影（简单把流型理解成空间即可），因此由 $\\hat X(t|t)$ 和 $X(t)$ 运算得到的 $\\widetilde X(t|t)$ 也属于这个流型，而 $W(t)$ 与这个流型正交，所以有 $W(t) \\bot \\widetilde X(t|t)$ ，那么在计算协方差更新公式时有 $$P(t+1|t) = E[\\widetilde X(t+1|t)] \\widetilde X^T(t+1|t)]$$ $$= E{ [\\Phi \\widetilde X(t|t) +\\Gamma W(t)][\\Phi \\widetilde X(t|t) +\\Gamma W(t)]^T}$$$$= E[\\Phi \\widetilde X(t|t) \\widetilde X^T(t|t) \\Phi] +E[\\Phi \\widetilde X(t|t) W^T(t) \\Gamma^T ]+E[\\Gamma W(t) \\widetilde X^T(t|t)\\Phi ]+E[\\Gamma W(t)W^T(t)\\Gamma^T]$$$$= \\Phi E[\\widetilde X(t|t) \\widetilde X^T(t|t)]\\Phi^T+0+0+\\Gamma E[W(t)W^T(t)]\\Gamma^T$$ $$= \\Phi P(t|t)\\Phi^T+\\Gamma Q \\Gamma^T$$ （真长(-｡-;)）同理对另一个协方差更新公式进行计算会得到$$P(t+1|t+1) =[I-K(t+1)H]P(t+1|t)[I-K(t+1)H]^T+K(t+1)R K^T(t+1)$$当然这个公式还不是最简的形式，还可以进一步化简，之前文中已经提到了K(t+1)的表达式，则$$K(t+1)=E[X(t+1)\\epsilon^T(t+1)]{ E[\\epsilon (t+1)\\epsilon^T(t+1)]}^{-1}$$ $$= E{[\\hat X(t+1|t)+\\widetilde X(t+1|t)][H\\widetilde X(t+1|t)+V(t+1)]^T} { E[\\epsilon (t+1)\\epsilon^T(t+1)]}^{-1}$$ $$= {E[\\hat X(t+1|t) \\widetilde X^T(t+1|t)H^T]+E[\\widetilde X(t+1|t)\\widetilde X^T(t+1|t)H^T]+E[\\hat X(t+1|t)V^T(t+1)]+E[ \\widetilde X(t+1|t) V^T(t+1)]} { E[\\epsilon (t+1)\\epsilon^T(t+1)]}^{-1}$$ $$=( 0+P(t+1|t)H^T+0+0){ E[\\epsilon (t+1)\\epsilon^T(t+1)]}^{-1}$$ $$=P(t+1|t)H^T E{ [H \\widetilde X(t+1|t)+V(t+1)][ H \\widetilde X(t+1|t)+V(t+1)]^T}^{-1}$$ $$=P(t+1|t)H^T { HP(t+1|t)H^T+R}^{-1}$$将这个表达式代入上面的协方差更新公式 (基于简化公式的考虑，下面的推导省略时标)$$P(t+1|t+1) = [I-KH]P[I-KH]^T+KRK^T$$ $$=[I-KH]P-PH^TK^T+K(HPH^T+R)K^T$$ $$=[I-KH]P-PH^TK^T+PH^TK^T$$ $$=[I-KH]P$$即$$P(t+1|t+1) = [I-K(t+1)H]P(t+1|t)$$至此所有推导已经完成了","link":"/2023/02/12/Kalman%20Filter%20II/"},{"title":"Lazy Loading","text":"I came across lazy loading while building a new feature for our company’s new website. In our page, we have tons of images to load, which is very costly and exerts a very negative user experience. Thus, we try to use lazy loading to improve the speed of loading the page.To build a twittmap, we need to utilize the twitter API. Here, I chose twitter4j, which is an unofficial Java library for the Twitter API. With Twitter4J, you can easily integrate your Java application with the Twitter service. There are some tutorials here: How to “Sign in with Twitter” using twitter4j.## Register twitter appClick this linkand fill out the following form, it should be fine. Just remember to grant your app access for read and write.## Include twitter4jFor those who are not familiar with java, it would be a painful process to learn how use new tools at first(I like VS better, lol). Don’t worry, just follow the steps below. Download twitter4j from here. Unzip it. After you create a java project, Right-click on your Java project and Select Properties. Choose Java Build Path and select the third tab, Libraries. Click add external library and add twitter4j-core-4.0.4.jar. Begin programmingA lot of examples are available. Here is the sample code used in tutorial mentioned above (Using eclipse). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package com.example.test;import twitter4j.Status;import twitter4j.Twitter;import twitter4j.TwitterException;import twitter4j.TwitterFactory;import twitter4j.auth.AccessToken;import twitter4j.auth.RequestToken;import twitter4j.conf.ConfigurationBuilder;import twitter4j.conf.Configuration; import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;public class FirstClass { public static void main(String[] args) { // TODO Auto-generated method stub String testStatus=&quot;Hello from twitter4j&quot;; ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;CONSUMER-KEY&quot;) .setOAuthConsumerSecret(&quot;CONSUMER-SECRET&quot;); try{ TwitterFactory tf = new TwitterFactory(cb.build()); Twitter twitter = tf.getInstance(); try{ RequestToken requestToken = twitter.getOAuthRequestToken(); System.out.println(&quot;Got request token.&quot;); System.out.println(&quot;Request token: &quot; + requestToken.getToken()); System.out.println(&quot;Request token secret: &quot; + requestToken.getTokenSecret()); AccessToken accessToken = null; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); while(accessToken == null){ System.out.println(&quot;Open the following URL and grant access to your account:&quot;); System.out.println(requestToken.getAuthorizationURL()); System.out.print(&quot;Enter the PIN(if available) and hit enter after you granted access.[PIN]:&quot;); String pin = br.readLine(); try { if (pin.length() &gt; 0) { accessToken = twitter.getOAuthAccessToken(requestToken, pin); } else { accessToken = twitter.getOAuthAccessToken(requestToken); } } catch (TwitterException te) { if (401 == te.getStatusCode()) { System.out.println(&quot;Unable to get the access token.&quot;); } else { te.printStackTrace(); } } } System.out.println(&quot;Got access token.&quot;); System.out.println(&quot;Access token: &quot; + accessToken.getToken()); System.out.println(&quot;Access token secret: &quot; + accessToken.getTokenSecret()); } catch(IllegalStateException ie){ if (!twitter.getAuthorization().isEnabled()) { System.out.println(&quot;OAuth consumer key/secret is not set.&quot;); System.exit(-1); } } Status status = twitter.updateStatus(testStatus); System.out.println(&quot;Successfully updated the status to [&quot; + status.getText() + &quot;].&quot;); System.exit(0); } catch (TwitterException te) { te.printStackTrace(); System.out.println(&quot;Failed to get timeline: &quot; + te.getMessage()); System.exit(-1); } catch (IOException ioe) { ioe.printStackTrace(); System.out.println(&quot;Failed to read the system input.&quot;); System.exit(-1); } }} This piece of code can post a twitter from your account, after running this u can see the following instructions.Just click into the url it mentioned and grant access to your app, and then type in the PIN code that it gives u. After that, u can check ur twitter now.## Use streaming APIHere is the code. 1234567891011121314151617181920212223242526272829303132333435363738394041package com.example.test;import twitter4j.*;import twitter4j.conf.ConfigurationBuilder;public class PrintSampleStream { public static void main(String[] args) throws TwitterException{ ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;I won't tell u&quot;) .setOAuthConsumerSecret(&quot;what my key and secret is&quot;) .setOAuthAccessToken(&quot;LOL&quot;) .setOAuthAccessTokenSecret(&quot;:)&quot;); TwitterStream twitterStream = new TwitterStreamFactory(cb.build()).getInstance(); StatusListener listener = new StatusListener() { public void onStatus(Status status) { System.out.println(status.getUser().getName() + &quot; : &quot; + status.getText()); } public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice) {} public void onTrackLimitationNotice(int numberOfLimitedStatuses) {} public void onException(Exception ex){ ex.printStackTrace(); } @Override public void onScrubGeo(long arg0, long arg1) { // TODO Auto-generated method stub } @Override public void onStallWarning(StallWarning arg0) { // TODO Auto-generated method stub } }; twitterStream.addListener(listener); twitterStream.sample(); }} This piece of sample code enables you to fetch tweets from twitter in real-time. Fetch Filtered StreamSometimes, we would like to search information that we are interested in, so we may need to filter the stream so that we can get relevant twitters. The code is shown below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.example.test;import twitter4j.*;import twitter4j.conf.ConfigurationBuilder;import java.util.ArrayList;import java.util.Arrays;public class PrintFilteredStream { public static void main(String[] args) throws TwitterException { String[] keywordsArray = {&quot;iphone&quot;, &quot;samsung&quot;}; ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;CONSUMER-KEY&quot;) .setOAuthConsumerSecret(&quot;CONSUMER-SECRET&quot;) .setOAuthAccessToken(&quot;ACCESS-TOKEN&quot;) .setOAuthAccessTokenSecret(&quot;ACCESS-TOKEN-SECRET&quot;); // Implementing StatusListner StatusListener listener = new StatusListener() { @Override public void onStatus(Status status) { System.out.println(status.getUser().getName() + &quot; : &quot; + status.getText()); } @Override public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice) { System.out.println(&quot;Got a status deletion notice id:&quot; + statusDeletionNotice.getStatusId()); } @Override public void onTrackLimitationNotice(int numberOfLimitedStatuses) { System.out.println(&quot;Got track limitation notice:&quot; + numberOfLimitedStatuses); } @Override public void onException(Exception ex){ ex.printStackTrace(); } @Override public void onScrubGeo(long userId, long upToStatusId) { System.out.println(&quot;Got scrub_geo event userId:&quot; + userId + &quot; upToStatusId:&quot; + upToStatusId); } @Override public void onStallWarning(StallWarning warning) { System.out.println(&quot;Got stall warning:&quot; + warning); } }; TwitterStream twitterStream = new TwitterStreamFactory(cb.build()).getInstance(); twitterStream.addListener(listener); ArrayList&lt;Long&gt; follow = new ArrayList&lt;Long&gt;(); ArrayList&lt;String&gt; track = new ArrayList&lt;String&gt;(); for(String arg : keywordsArray){ if(isNumericalArgument(arg)){ for(String id : arg.split(&quot;,&quot;)){ follow.add(Long.parseLong(id)); } }else{ track.addAll(Arrays.asList(arg.split(&quot;,&quot;))); } } long[] followArray = new long[follow.size()]; for(int i = 0; i &lt; follow.size(); i++){ followArray[i] = follow.get(i); } String[] trackArray = track.toArray(new String[track.size()]); twitterStream.filter(new FilterQuery(0, followArray, trackArray)); } private static boolean isNumericalArgument(String argument){ String args[] = argument.split(&quot;,&quot;); boolean isNumericalArgument = true; for(String arg : args){ try{ Integer.parseInt(arg); } catch(NumberFormatException nfe){ isNumericalArgument = false; break; } } return isNumericalArgument; }} Set up elasticsearch environment Go to AWS console and select elasticsearch service(you can also choose to use elasticsearch by downloading and unzipping it from here) Set access police to open access or any way you like, and then use curl to create index using index API.12345678$ curl -XPUT 'http://search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/' -d '{ &quot;settings&quot; : { &quot;index&quot; : { &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2 } }}' this piece of code will create an index named “user” in your AWS domain. Next, you are going to create mapping. Here, the mapping called “profile” is created, which has three properties: text, latitude and longitude.123456789101112curl -XPOST search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/_mapping/profile -d '{ &quot;profile&quot; : { &quot;properties&quot; : { &quot;text&quot; : { &quot;type&quot; : &quot;string&quot;}, &quot;latitude&quot; : { &quot;type&quot; : &quot;string&quot;}, &quot;longitude&quot; : { &quot;type&quot; : &quot;string&quot;} } }}' Upload data to elasticsearchTo upload data to elasticsearch, you can use the curl command in cygwin, since I am using a windows machine, or put this piece of code into the java program that are fetching twitter. From twitter stream, we can obtain a JSONObject, which could by utilize into uploading to elasticsearch. 12345678910111213141516String url = &quot;http://search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/profile&quot;;HttpClient client = HttpClientBuilder.create().build();HttpPost put = new HttpPost(url);put.setHeader(&quot;Content-type&quot;, &quot;application/json&quot;);StringEntity params =new StringEntity(json.toString());put.setEntity(params);HttpResponse response = client.execute(put);System.out.println(&quot;Response Code:&quot;+response.getStatusLine().getStatusCode()); BufferedReader rd = new BufferedReader(new InputStreamReader(response.getEntity().getContent()));StringBuffer result = new StringBuffer();String line = &quot;&quot;;while ((line = rd.readLine()) != null) { result.append(line);} System.out.println(&quot;result:&quot;+result); UI for our applicationThis is the entry point for our application, you can choose keyword for twitter.After you choose the keyword and click the “show” button, you can see the corresponding twitter location marked on the map.Every 5 seconds, the front end will refresh and the counter, which can be seen in the second picture as 10, will gradually increase as our backend program continuously fetching data from twitter.","link":"/2018/05/24/Lazy%20Loading/"},{"title":"Kalman Filter I(Chinese)","text":"最近利用卡尔曼滤波来做tracking的东西，然后随手推导了一下。网上大都是只有例子源代码和约定俗成的公式，不太方便使用，毕竟不能保证你遇到的问题恰好能用现有模型来套。所以对滤波器的来由需要深刻地理解一下。 （一）Kalman Filter原理定义1.1由 $m\\times 1$ 维随机向量 $y\\in R^m$ 的线性函数估计 $n\\times 1$ 维随机变量 $x\\in R^n$ ，估计值记为$$ \\hat x = b + Ay ,b\\in R^n,A\\in R^{n\\times m}$$若估计值极小化的指标函数为$$ J = E[(x- \\hat x)^T(x- \\hat x)]$$则称 $\\hat x$ 为随机变量的线性最小方差估计。由观测值 $y$ 求随机变量 $x$ 的线性最小方差估计的表达式为$$\\hat x = E(x)+Cov(x,y)Var(y)^{-1}(y-E(y))$$虽然这个公式好像好难懂，不过暂时不需要完全理解它，后面在推导时可以慢慢领会。同时这个 $\\hat x$ 有三个性质 （1）无偏性，$E(\\hat x) = E(x)$ （2）正交性，$E[(x-\\hat x)y^T] = 0$ （3）不相关性，$x-\\hat x$ 与 $y$ 不相关 定义1.2 $x-\\hat x$ 与 $y$ 不相关，那么等价于 $x-\\hat x$ 与 $y$ 正交（或者理解为垂直），记为 $x-\\hat x\\bot y$ ，并且称$ \\hat x $为 $x$ 在 $y$ 上 的射影，记为$\\hat x = proj（x | y） $ 定义1.3基于随机变量 $y(1),y(2),…,y(k)\\in R^m$，对随机变量 $x\\in R^m$ 的线性最小方差估计 $ \\hat x$ 定义为$$\\hat x = proj(x|w)=proj(x|y(1),y(2),…,y(k))$$也称 $\\hat x$ 为 $x$ 在线性流型 $L(w)$ 或 $L(y(1),y(2),…,y(k))$ 上的射影。流型的概念不需要理解，因为后面推导不涉及对它的完全理解## 定义1.4设 $y(1),y(2),…,y(k)\\in R^m$ 是存在二阶矩的随机序列，它的新息序列（不用纠结名字，其实后面推导里它就是误差序列）定义为$$\\epsilon(k)=y(k)-proj(y(k)|y(1),y(2),…,y(k-1)),k =1,2,…$$并定义的一步最优预报估计值为$$\\hat y(k|k-1)=proj(y(k)|y(1),y(2),…,y(k-1))$$因此新息序列可重新写成$$\\epsilon(k)=y(k)-\\hat y(k|k-1),k =1,2,…$$需要规定 $\\hat y(1|0)=E[y(1)]$ ，这保证了$E[\\epsilon(1)]=0$，所以有$\\epsilon(k) \\bot L(y(1),y(2),…,y(k-1))$## 定义1.5设随机变量 $x\\in R^n$ ，随机序列 $y(k)\\in R^m$ ，而且随机序列存在二阶矩，则有递推公式（证明以后再补好了(-｡-;)）$$proj(x|y(1),y(2),…,y(k))=proj(x|y(1),y(2),…,y(k-1))+E[x\\epsilon^T(k)]{E[\\epsilon(k)\\epsilon^T(k)]}^{-1}\\epsilon(k)$$","link":"/2023/02/12/Kalman%20Filter%20I/"},{"title":"Derangement of An Array","text":"Suppose that there are n people who are numbered 1, 2, …, n. Let there be n hats also numbered 1, 2, …, n. We have to find the number of ways in which no one gets the hat having same number as their number. Let us assume that the first person takes hat i. There are n − 1 ways for the first person to make such a choice. There are now two possibilities, depending on whether or not person i takes hat 1 in return: Person i does not take the hat 1. This case is equivalent to solving the problem with n − 1 persons and n − 1 hats: each of the remaining n − 1 people has precisely 1 forbidden choice from among the remaining n − 1 hats (i’s forbidden choice is hat 1). Person i takes the hat 1. Now the problem reduces to n − 2 persons and n − 2 hats. 1234567891011class Solution {public: int findDerangement(int n) { int64_t mod = pow(10, 9) + 7; vector&lt;int64_t&gt; buf(n + 1, 0); buf[1] = 0, buf[2] = 1; for (int i = 3; i &lt;= n; i++) buf[i] = ((int64_t)(i - 1) * (buf[i - 1] + buf[i - 2])) % mod; return (int)buf[n]; }};","link":"/2023/02/12/LeetCode-634-DerangementOfAnArray/"},{"title":"Two Sum","text":"What’s interesting is that if I try to put the number into the unordered_map before I check for numbers that satisfy my target, some test cases won’t pass. 1234567891011121314151617class Solution {public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { unordered_map&lt;int, int&gt; buf; vector&lt;int&gt; res; for (int i = 0; i &lt; nums.size(); i ++) { if (buf.count(target - nums[i]) &gt; 0 &amp;&amp; i != buf[target - nums[i]]) { res.push_back(i); res.push_back(buf[target - nums[i]]); break; } buf[nums[i]] = i; } sort(res.begin(), res.end()); return res; }};","link":"/2023/02/12/LeetCode-1/"},{"title":"Monitoring System Tech Stack","text":"some loggerinfluxdb, grafana, kapacitor, pagerduty, kibana Tracingzipkin, stores traces in elasticsearch Probingpokey, rigor","link":"/2018/06/13/Monitoring%20System%20Tech%20Stack/"},{"title":"Virtual Memory(Draft)","text":"Limitations of Prvious Memory Management TechniquesIntroduction to Virtual MemoryThe basic idea is that each program has its own address space, which would be broken up into chunks called pages. In other words, a page is just a contiguous range of addresses that can be mapped onto physical memory. But not all pages have to be in memory at the same time Pagingpages are the the fixed-size unit of the virtual address space, and their corresponding units in the physical memory are called page frames. Normally, the size is 4KB, but larger page like 2MB, 1GB do exists. Transfers between RAM and disk are always in whole pages. When an instruction is executed, say 1MOV REG, 0 the address 0, which is genereated by user application would be sent to Memory Management Unit(MMU), which would map it into real address in RAM. The mapping that MMU uses is stored in the page table. Once the instruction references an unmapped address, the MMU notices that and causes the CPU to trap to the operating system. This trap is called a page fault. The operating system picks a little-used page frame and writes its contents back to the disk (if it is not already there). It then fetches (also from the disk) the page that was just referenced into the page frame just freed, changes the map, and restarts the trapped instruction Page Table and Translation Lookaside BuffersConside a 1-byte instruction that copies one register to another. In the absence of paging, this instruction makes only one memory reference, to fetch the instruction. With paging, at least one additional memory reference will be needed, to access the pagetable. Having to make two memory references per memory reference reduces performance by half. The solution to the problem is to equip computers with a small hardware for mapping addresses without going through the page table, which is based on the assumption that most programs tend to make a large number of references to a small number of pages. This device is called Translation Lookaside Buffer(TLB) or associative memory. Normally, TLB is inside MMU and it can check for address tuples in parallel Page Replacement Algorithm NRU(Not Recently Used): Easy to understand Moderately efficient to implement Adequate performance FIFO(First-In, First-Out): Rarely used Second-Chance: Degenerate into FIFO when all pages have been referenced Clock Similar to Second-Chance, only in this case the list is circular LRU(Least Recently Used): Very expensive An ordered linked list of all pages in memory NFU(Not Frequently Used): One page heavily used may outweigh other useful pages Aging Figure out which page is referenced last if they both are used with the same frequency The Working Set: Keep track of each process’s working set of pages and make sure it is in memory before running Most programs randomly access a small number of pages, but the working set of pages changes slowly in time Never used slide window of memory references due to performance issues, use slide window of time Expensive WSClock Widely used in practice Efficient","link":"/2018/06/12/Memory%20Management/"},{"title":"Object Oriented Programming(Draft)","text":"This article is full of shit that I don’t understand:( Static Class Based ModelThe class represents a formal abstract set of the generalized characteristics of an instance (the knowledge about objects). Characteristics of instances are: properties (object description) and methods (object activity). Prototype Based Modelobjects can independently store all their characteristics (properties, methods) and do not need the class. DelegationIn contrast with the static class based implementation, in case of impossibility to respond the message, the conclusion is: the object at the moment does not have the requested characteristic, however to get the result is still possible if to try to analyze alternative prototype chain, or probably, the object will have such characteristic after a number of mutations. Concatenative ModelConcatenative prototyping is to not use delegation but exact copy of a prototype at the moment of object creation Duck Typingidentification of objects can be made not by their hierarchy and belonging to concrete type, but by a current set of characteristics. EncapsulationThe basic purpose of the encapsulation, repeat, is an abstraction from the user of the auxiliary helper data and not a “way to secure object from hackers”. Encapsulating auxiliary helper (local) objects, we provide possibility for further behavior changes of the public-interface with a minimum of expenses, localizing and predicting places of these changes. And exactly this is the main encapsulation purpose. MixinsMixins have been suggested as an alternative to multiple inheritance. These are independent elements which can be mixed with any objects, extending their functionality.","link":"/2018/07/05/Object%20Oriented%20Programming/"},{"title":"Processes and Threads","text":"ProcessIn UNIX, there is only one system call to create a new process: fork . This callcreates an exact clone of the calling process. After the fork , the two processes, theparent and the child, have the same memory image, the same environment strings,and the same open files. In UNIX, a process and all of its children and further descendants togetherform a process group. When a user sends a signal from the keyboard, the signal isdelivered to all members of the process group currently associated with thekeyboard In Unix, all the processes in the whole system be-long to a single tree, with init at the root. In contrast, Windows has no concept of a process hierarchy. All processes areequal. ThreadReasons for having threads: in manyapplications, multiple activities are going on at once. Some of these may blockfrom time to time. By decomposing such an application into multiple sequentialthreads that run in quasi-parallel, the programming model becomes simpler.(It is very verbose!) they are lighter weight thanprocesses, they are easier (i.e., faster) to create and destroy than processes. Threadsyield no performance gain when all of them are CPU bound, but when there is sub-stantial computing and also substantial I/O, having threads allows these activitiesto overlap, thus speeding up the application.","link":"/2018/06/07/Processes%20and%20Threads/"},{"title":"","text":"blog-postsAll my blog posts","link":"/2023/02/12/README/"},{"title":"Regex Engine Implementation(Draft)","text":"IntroductionThis blog posts originate from a leetcode probelm: wildcard matching. By that time, I know very little about regex, so solving the problem brings me unspeakable frustration ：(. However, after graduation, I take a closer look into the concept and implementation of regex in my spare time and this blog mainly records what I have discovered so far. Traditional Backtracking ApproachI have the implementation in my toy repo Dynamic Programming ApproachThis approach is mainly from leetcode, working on a limited set of regex meta-characters like . and *. Finite Automata ApproachI came across this approach in Russ Cox’s article: Regular Expression Matching Can Be Simple And Fast. To begin with, we need to know what finite automata is. DFA and NFAFinite automata are also known as state machines, which can be represented in the diagram by circles. Each character is modeled as a state(represented as circle) and they are linked directionaly. If each possible input character leads to at most one new state, the whole machine(think of it as the set of all states and the execution process) is called a deterministic finite automation(DFA). The counterpart of DFA is non-deterministic finite automata(NFA or NDFA). Converting Regex to NFAsThese two things turn out to be exactly equivalent in power: they can be converted to each other. Multiple ways exist for doing the conversion, but in here we’ll only describe the method used in the paper, which is Thompson’s approach. It is pretty easy to understand from the paper, where each NFA has no matching state but a dangling arrow. ImplementationFirst, we need to declare an NFA: 123456789// c &lt; 256: one character// c = 256: split// c = 257: matchedstruct State { int c; State *out; State *out1; int lastlist;}","link":"/2018/07/15/Regex%20Engine%20Implementation/"},{"title":"Reproducing Kernel Hilbert Space","text":"Before we get into the magical term reproducing kernel Hilbert Space(RKHS), there are several term we need to know. Fields","link":"/2018/06/11/Reproducing%20Kernel%20Hilbert%20Space/"},{"title":"Segmentation(Draft)","text":"Why SegmentationConsider a compiler as an example, it has several tables as compilation proceeds, such as the symbol table, the parse tree table, the stack used for procedure calls and etc.;If one of the table is exceptionally large, we would want it to be managed dynamically, in other words, a way of freeing the programmer from having to manage the expanding and contracting tables. That’s what segmentation is for. What is SegmentationSegmentation is a linear sequence of addresses.","link":"/2018/06/18/Segmentation/"},{"title":"Introduction to Spark","text":"OverviewSpark that supports these applications while retaining the scalability and fault tolerance ofMapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost.## Traditional SystemsTraditional systems like Dryad and MapReduce s achieve their scalability and fault toleranceby providing a programming model where the user creates acyclic data flow graphs to pass input data through a set of operators. However, they show limitation in cases as follows: Iterative job: Many common machine learning algorithms apply a function repeatedly to the same dataset to optimize a parameter. While each iteration can be expressed as a MapReduce/Dryad job, each job must reload the data from disk, incurring a significant performance penalty. Interactive analysis: Hadoop is often used to perform ad-hoc exploratory queries on big datasets, through SQL interfaces such as Pig and Hive. Ideally, a user would be able to load a dataset of interest into memory across a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant latency (tens of seconds) because it runs as a separate MapReduce job and reads data from disk.## Programming Model Spark provides two main abstractions for parallel programming: resilient distributed data-sets and parallel operations on these datasets (invoked by passing a function to apply on a data-set). Resilient Distributed Datasets (RDDs)A resilient distributed dataset (RDD) is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. In Spark, each RDD is represented by a Scala object. Spark lets programmers construct RDDs in four ways: From a file in a shared file system, such as the Hadoop Distributed File System (HDFS). By “parallelizing” a Scala collection (e.g., an array) in the driver program, which means dividing it into a number of slices that will be sent to multiple nodes. By transforming an existing RDD. A dataset with elements of type A can be transformed into a dataset with elements of type B using an operation called flatMap, which passes each element through a user-provided function of type A ⇒ List[B]. By changing the persistence of an existing RDD. By default, RDDs are lazy and ephemeral. That is, partitions of a dataset are materialized on demand when they are used in a parallel operation (e.g., by passing a block of a file through a map function), and are discarded from memory after use.However, a user can alter the persistence of an RDD through two actions: The $cache$ action leaves the dataset lazy, but hints that it should be kept in memory after the first time it is computed, because it will be reused.The $save$ action evaluates the dataset and writes it to a distributed filesystem such as HDFS. The saved version is used in future operations on it. Parallel Operations reduce:Combines dataset elements using an associativefunction to produce a result at the driver program collect:Sends all elements of the dataset to the driverprogram foreach:Passes each element through a user providedfunction. Shared VariablesProgrammers invoke operations like map, filter and reduceby passing closures (functions) to Spark. As is typical in functional programming, these closures can refer to variables in the scope where they are created. Normally, when Spark runs a closure on a worker node, these variables are copied to the worker. However, Spark also lets programmers create two restricted types of shared variables to support two simple but common usage patterns: Broadcast variables: If a large read-only piece of data (e.g., a lookup table) is used in multiple parallel operations, it is preferable to distribute it to the workers only once instead of packaging it with every closure. Accumulators: These are variables that workers can only “add” to using an associative operation, and that only the driver can read. They can be used to implement counters as in MapReduce and to provide a more imperative syntax for parallel sums. Accumulators can be defined for any type that has an “add” operation and a “zero” value. Due to their “add-only” semantics, they are easy to make fault-tolerant.## ImplementationSpark is built on top of Mesos and the core of Spark is the implementation of resilient distributeddatasets, which will be stored as a chain of objects capturing the lineage of each RDD When a parallel operation is invoked on a dataset, Spark creates a task to process each partition of the dataset and sends these tasks to worker nodes. Finally, shipping tasks to workers requires shipping closures to them—both the closures used to define a distributed dataset, and closures passed to operations. The two types of shared variables in Spark, broadcast variables and accumulators, are implemented using classes with custom serialization formats. HDFS is used to broadcast variables, while accumulators are implemented using a different “serialization trick.”","link":"/2017/08/28/Spark/"},{"title":"Twitter Map","text":"To build a twittmap, we need to utilize the twitter API. Here, I chose twitter4j, which is an unofficial Java library for the Twitter API. With Twitter4J, you can easily integrate your Java application with the Twitter service. There are some tutorials here: How to “Sign in with Twitter” using twitter4j.## Register twitter appClick this linkand fill out the following form, it should be fine. Just remember to grant your app access for read and write.## Include twitter4jFor those who are not familiar with java, it would be a painful process to learn how use new tools at first(I like VS better, lol). Don’t worry, just follow the steps below. Download twitter4j from here. Unzip it. After you create a java project, Right-click on your Java project and Select Properties. Choose Java Build Path and select the third tab, Libraries. Click add external library and add twitter4j-core-4.0.4.jar. Begin programmingA lot of examples are available. Here is the sample code used in tutorial mentioned above (Using eclipse). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package com.example.test;import twitter4j.Status;import twitter4j.Twitter;import twitter4j.TwitterException;import twitter4j.TwitterFactory;import twitter4j.auth.AccessToken;import twitter4j.auth.RequestToken;import twitter4j.conf.ConfigurationBuilder;import twitter4j.conf.Configuration; import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;public class FirstClass { public static void main(String[] args) { // TODO Auto-generated method stub String testStatus=&quot;Hello from twitter4j&quot;; ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;CONSUMER-KEY&quot;) .setOAuthConsumerSecret(&quot;CONSUMER-SECRET&quot;); try{ TwitterFactory tf = new TwitterFactory(cb.build()); Twitter twitter = tf.getInstance(); try{ RequestToken requestToken = twitter.getOAuthRequestToken(); System.out.println(&quot;Got request token.&quot;); System.out.println(&quot;Request token: &quot; + requestToken.getToken()); System.out.println(&quot;Request token secret: &quot; + requestToken.getTokenSecret()); AccessToken accessToken = null; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); while(accessToken == null){ System.out.println(&quot;Open the following URL and grant access to your account:&quot;); System.out.println(requestToken.getAuthorizationURL()); System.out.print(&quot;Enter the PIN(if available) and hit enter after you granted access.[PIN]:&quot;); String pin = br.readLine(); try { if (pin.length() &gt; 0) { accessToken = twitter.getOAuthAccessToken(requestToken, pin); } else { accessToken = twitter.getOAuthAccessToken(requestToken); } } catch (TwitterException te) { if (401 == te.getStatusCode()) { System.out.println(&quot;Unable to get the access token.&quot;); } else { te.printStackTrace(); } } } System.out.println(&quot;Got access token.&quot;); System.out.println(&quot;Access token: &quot; + accessToken.getToken()); System.out.println(&quot;Access token secret: &quot; + accessToken.getTokenSecret()); } catch(IllegalStateException ie){ if (!twitter.getAuthorization().isEnabled()) { System.out.println(&quot;OAuth consumer key/secret is not set.&quot;); System.exit(-1); } } Status status = twitter.updateStatus(testStatus); System.out.println(&quot;Successfully updated the status to [&quot; + status.getText() + &quot;].&quot;); System.exit(0); } catch (TwitterException te) { te.printStackTrace(); System.out.println(&quot;Failed to get timeline: &quot; + te.getMessage()); System.exit(-1); } catch (IOException ioe) { ioe.printStackTrace(); System.out.println(&quot;Failed to read the system input.&quot;); System.exit(-1); } }} This piece of code can post a twitter from your account, after running this u can see the following instructions.Just click into the url it mentioned and grant access to your app, and then type in the PIN code that it gives u. After that, u can check ur twitter now.## Use streaming APIHere is the code. 1234567891011121314151617181920212223242526272829303132333435363738394041package com.example.test;import twitter4j.*;import twitter4j.conf.ConfigurationBuilder;public class PrintSampleStream { public static void main(String[] args) throws TwitterException{ ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;I won't tell u&quot;) .setOAuthConsumerSecret(&quot;what my key and secret is&quot;) .setOAuthAccessToken(&quot;LOL&quot;) .setOAuthAccessTokenSecret(&quot;:)&quot;); TwitterStream twitterStream = new TwitterStreamFactory(cb.build()).getInstance(); StatusListener listener = new StatusListener() { public void onStatus(Status status) { System.out.println(status.getUser().getName() + &quot; : &quot; + status.getText()); } public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice) {} public void onTrackLimitationNotice(int numberOfLimitedStatuses) {} public void onException(Exception ex){ ex.printStackTrace(); } @Override public void onScrubGeo(long arg0, long arg1) { // TODO Auto-generated method stub } @Override public void onStallWarning(StallWarning arg0) { // TODO Auto-generated method stub } }; twitterStream.addListener(listener); twitterStream.sample(); }} This piece of sample code enables you to fetch tweets from twitter in real-time. Fetch Filtered StreamSometimes, we would like to search information that we are interested in, so we may need to filter the stream so that we can get relevant twitters. The code is shown below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.example.test;import twitter4j.*;import twitter4j.conf.ConfigurationBuilder;import java.util.ArrayList;import java.util.Arrays;public class PrintFilteredStream { public static void main(String[] args) throws TwitterException { String[] keywordsArray = {&quot;iphone&quot;, &quot;samsung&quot;}; ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;CONSUMER-KEY&quot;) .setOAuthConsumerSecret(&quot;CONSUMER-SECRET&quot;) .setOAuthAccessToken(&quot;ACCESS-TOKEN&quot;) .setOAuthAccessTokenSecret(&quot;ACCESS-TOKEN-SECRET&quot;); // Implementing StatusListner StatusListener listener = new StatusListener() { @Override public void onStatus(Status status) { System.out.println(status.getUser().getName() + &quot; : &quot; + status.getText()); } @Override public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice) { System.out.println(&quot;Got a status deletion notice id:&quot; + statusDeletionNotice.getStatusId()); } @Override public void onTrackLimitationNotice(int numberOfLimitedStatuses) { System.out.println(&quot;Got track limitation notice:&quot; + numberOfLimitedStatuses); } @Override public void onException(Exception ex){ ex.printStackTrace(); } @Override public void onScrubGeo(long userId, long upToStatusId) { System.out.println(&quot;Got scrub_geo event userId:&quot; + userId + &quot; upToStatusId:&quot; + upToStatusId); } @Override public void onStallWarning(StallWarning warning) { System.out.println(&quot;Got stall warning:&quot; + warning); } }; TwitterStream twitterStream = new TwitterStreamFactory(cb.build()).getInstance(); twitterStream.addListener(listener); ArrayList&lt;Long&gt; follow = new ArrayList&lt;Long&gt;(); ArrayList&lt;String&gt; track = new ArrayList&lt;String&gt;(); for(String arg : keywordsArray){ if(isNumericalArgument(arg)){ for(String id : arg.split(&quot;,&quot;)){ follow.add(Long.parseLong(id)); } }else{ track.addAll(Arrays.asList(arg.split(&quot;,&quot;))); } } long[] followArray = new long[follow.size()]; for(int i = 0; i &lt; follow.size(); i++){ followArray[i] = follow.get(i); } String[] trackArray = track.toArray(new String[track.size()]); twitterStream.filter(new FilterQuery(0, followArray, trackArray)); } private static boolean isNumericalArgument(String argument){ String args[] = argument.split(&quot;,&quot;); boolean isNumericalArgument = true; for(String arg : args){ try{ Integer.parseInt(arg); } catch(NumberFormatException nfe){ isNumericalArgument = false; break; } } return isNumericalArgument; }} Set up elasticsearch environment Go to AWS console and select elasticsearch service(you can also choose to use elasticsearch by downloading and unzipping it from here) Set access police to open access or any way you like, and then use curl to create index using index API.12345678$ curl -XPUT 'http://search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/' -d '{ &quot;settings&quot; : { &quot;index&quot; : { &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2 } }}' this piece of code will create an index named “user” in your AWS domain. Next, you are going to create mapping. Here, the mapping called “profile” is created, which has three properties: text, latitude and longitude.123456789101112curl -XPOST search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/_mapping/profile -d '{ &quot;profile&quot; : { &quot;properties&quot; : { &quot;text&quot; : { &quot;type&quot; : &quot;string&quot;}, &quot;latitude&quot; : { &quot;type&quot; : &quot;string&quot;}, &quot;longitude&quot; : { &quot;type&quot; : &quot;string&quot;} } }}' Upload data to elasticsearchTo upload data to elasticsearch, you can use the curl command in cygwin, since I am using a windows machine, or put this piece of code into the java program that are fetching twitter. From twitter stream, we can obtain a JSONObject, which could by utilize into uploading to elasticsearch. 12345678910111213141516String url = &quot;http://search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/profile&quot;;HttpClient client = HttpClientBuilder.create().build();HttpPost put = new HttpPost(url);put.setHeader(&quot;Content-type&quot;, &quot;application/json&quot;);StringEntity params =new StringEntity(json.toString());put.setEntity(params);HttpResponse response = client.execute(put);System.out.println(&quot;Response Code:&quot;+response.getStatusLine().getStatusCode()); BufferedReader rd = new BufferedReader(new InputStreamReader(response.getEntity().getContent()));StringBuffer result = new StringBuffer();String line = &quot;&quot;;while ((line = rd.readLine()) != null) { result.append(line);} System.out.println(&quot;result:&quot;+result); UI for our applicationThis is the entry point for our application, you can choose keyword for twitter.After you choose the keyword and click the “show” button, you can see the corresponding twitter location marked on the map.Every 5 seconds, the front end will refresh and the counter, which can be seen in the second picture as 10, will gradually increase as our backend program continuously fetching data from twitter.","link":"/2017/08/28/Twitter%20Map/"},{"title":"Two-phase Commit","text":"IntroductionTwo-phase commit(2PC) is an atomic commitment protocol, which is related to but different from consensus, and is widely used to implemente distributed transactions. Protocol A coordinator sends a proposal to all participants. When a participant receives a proposal, it responds with a “Yes” or “No”. If the participants votes “No”, it can immediately abort. The coordinator collects votes, if all were “Yes”, the coordinator decides to commit and sends “Commit” message to each participant; otherwise, it will send “Abort” message to each participant that voted “Yes” Participants wait for message from the coordinator. When they receive one, they should act accordingly. From my perspective, the protocol itself is already very similar to paxos. TimeoutsThe words from the ppt is so mysterious, I couldn’t interpret it :( FailoverConclusion 2PC guarantees consistency but not availablity.","link":"/2018/07/13/Two-phase%20Commit/"},{"title":"Using React Component in Twig File","text":"Let’s start with some key concepts like what twig is, how twig work and etc., and then we can move on to the section of how to use react component in twig file.## TwigSo twig, first and foremost, is a template engine for PHP(I don’t really like it but it is official language in the company), which takes data(handled by backend logic) and template(frontend files like .html) as input and compiles them into the pages. There are four steps in total: 12345678Load the template and if the template is already compiled load it else the lexer tokenize the code into smaller pieces the parser converts the token stream into a tree of nodes(AST) the compiler transform the AST into php codeCalled display method a more detailed of the whole process can be found here. To build a twittmap, we need to utilize the twitter API. Here, I chose twitter4j, which is an unofficial Java library for the Twitter API. With Twitter4J, you can easily integrate your Java application with the Twitter service. There are some tutorials here: How to “Sign in with Twitter” using twitter4j.## Register twitter appClick this linkand fill out the following form, it should be fine. Just remember to grant your app access for read and write.## Include twitter4jFor those who are not familiar with java, it would be a painful process to learn how use new tools at first(I like VS better, lol). Don’t worry, just follow the steps below. Download twitter4j from here. Unzip it. After you create a java project, Right-click on your Java project and Select Properties. Choose Java Build Path and select the third tab, Libraries. Click add external library and add twitter4j-core-4.0.4.jar. Begin programmingA lot of examples are available. Here is the sample code used in tutorial mentioned above (Using eclipse). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package com.example.test;import twitter4j.Status;import twitter4j.Twitter;import twitter4j.TwitterException;import twitter4j.TwitterFactory;import twitter4j.auth.AccessToken;import twitter4j.auth.RequestToken;import twitter4j.conf.ConfigurationBuilder;import twitter4j.conf.Configuration; import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;public class FirstClass { public static void main(String[] args) { // TODO Auto-generated method stub String testStatus=&quot;Hello from twitter4j&quot;; ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;CONSUMER-KEY&quot;) .setOAuthConsumerSecret(&quot;CONSUMER-SECRET&quot;); try{ TwitterFactory tf = new TwitterFactory(cb.build()); Twitter twitter = tf.getInstance(); try{ RequestToken requestToken = twitter.getOAuthRequestToken(); System.out.println(&quot;Got request token.&quot;); System.out.println(&quot;Request token: &quot; + requestToken.getToken()); System.out.println(&quot;Request token secret: &quot; + requestToken.getTokenSecret()); AccessToken accessToken = null; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); while(accessToken == null){ System.out.println(&quot;Open the following URL and grant access to your account:&quot;); System.out.println(requestToken.getAuthorizationURL()); System.out.print(&quot;Enter the PIN(if available) and hit enter after you granted access.[PIN]:&quot;); String pin = br.readLine(); try { if (pin.length() &gt; 0) { accessToken = twitter.getOAuthAccessToken(requestToken, pin); } else { accessToken = twitter.getOAuthAccessToken(requestToken); } } catch (TwitterException te) { if (401 == te.getStatusCode()) { System.out.println(&quot;Unable to get the access token.&quot;); } else { te.printStackTrace(); } } } System.out.println(&quot;Got access token.&quot;); System.out.println(&quot;Access token: &quot; + accessToken.getToken()); System.out.println(&quot;Access token secret: &quot; + accessToken.getTokenSecret()); } catch(IllegalStateException ie){ if (!twitter.getAuthorization().isEnabled()) { System.out.println(&quot;OAuth consumer key/secret is not set.&quot;); System.exit(-1); } } Status status = twitter.updateStatus(testStatus); System.out.println(&quot;Successfully updated the status to [&quot; + status.getText() + &quot;].&quot;); System.exit(0); } catch (TwitterException te) { te.printStackTrace(); System.out.println(&quot;Failed to get timeline: &quot; + te.getMessage()); System.exit(-1); } catch (IOException ioe) { ioe.printStackTrace(); System.out.println(&quot;Failed to read the system input.&quot;); System.exit(-1); } }} This piece of code can post a twitter from your account, after running this u can see the following instructions.Just click into the url it mentioned and grant access to your app, and then type in the PIN code that it gives u. After that, u can check ur twitter now.## Use streaming APIHere is the code. 1234567891011121314151617181920212223242526272829303132333435363738394041package com.example.test;import twitter4j.*;import twitter4j.conf.ConfigurationBuilder;public class PrintSampleStream { public static void main(String[] args) throws TwitterException{ ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;I won't tell u&quot;) .setOAuthConsumerSecret(&quot;what my key and secret is&quot;) .setOAuthAccessToken(&quot;LOL&quot;) .setOAuthAccessTokenSecret(&quot;:)&quot;); TwitterStream twitterStream = new TwitterStreamFactory(cb.build()).getInstance(); StatusListener listener = new StatusListener() { public void onStatus(Status status) { System.out.println(status.getUser().getName() + &quot; : &quot; + status.getText()); } public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice) {} public void onTrackLimitationNotice(int numberOfLimitedStatuses) {} public void onException(Exception ex){ ex.printStackTrace(); } @Override public void onScrubGeo(long arg0, long arg1) { // TODO Auto-generated method stub } @Override public void onStallWarning(StallWarning arg0) { // TODO Auto-generated method stub } }; twitterStream.addListener(listener); twitterStream.sample(); }} This piece of sample code enables you to fetch tweets from twitter in real-time. Fetch Filtered StreamSometimes, we would like to search information that we are interested in, so we may need to filter the stream so that we can get relevant twitters. The code is shown below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.example.test;import twitter4j.*;import twitter4j.conf.ConfigurationBuilder;import java.util.ArrayList;import java.util.Arrays;public class PrintFilteredStream { public static void main(String[] args) throws TwitterException { String[] keywordsArray = {&quot;iphone&quot;, &quot;samsung&quot;}; ConfigurationBuilder cb = new ConfigurationBuilder(); //the following is set without accesstoken- desktop client cb.setDebugEnabled(true) .setOAuthConsumerKey(&quot;CONSUMER-KEY&quot;) .setOAuthConsumerSecret(&quot;CONSUMER-SECRET&quot;) .setOAuthAccessToken(&quot;ACCESS-TOKEN&quot;) .setOAuthAccessTokenSecret(&quot;ACCESS-TOKEN-SECRET&quot;); // Implementing StatusListner StatusListener listener = new StatusListener() { @Override public void onStatus(Status status) { System.out.println(status.getUser().getName() + &quot; : &quot; + status.getText()); } @Override public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice) { System.out.println(&quot;Got a status deletion notice id:&quot; + statusDeletionNotice.getStatusId()); } @Override public void onTrackLimitationNotice(int numberOfLimitedStatuses) { System.out.println(&quot;Got track limitation notice:&quot; + numberOfLimitedStatuses); } @Override public void onException(Exception ex){ ex.printStackTrace(); } @Override public void onScrubGeo(long userId, long upToStatusId) { System.out.println(&quot;Got scrub_geo event userId:&quot; + userId + &quot; upToStatusId:&quot; + upToStatusId); } @Override public void onStallWarning(StallWarning warning) { System.out.println(&quot;Got stall warning:&quot; + warning); } }; TwitterStream twitterStream = new TwitterStreamFactory(cb.build()).getInstance(); twitterStream.addListener(listener); ArrayList&lt;Long&gt; follow = new ArrayList&lt;Long&gt;(); ArrayList&lt;String&gt; track = new ArrayList&lt;String&gt;(); for(String arg : keywordsArray){ if(isNumericalArgument(arg)){ for(String id : arg.split(&quot;,&quot;)){ follow.add(Long.parseLong(id)); } }else{ track.addAll(Arrays.asList(arg.split(&quot;,&quot;))); } } long[] followArray = new long[follow.size()]; for(int i = 0; i &lt; follow.size(); i++){ followArray[i] = follow.get(i); } String[] trackArray = track.toArray(new String[track.size()]); twitterStream.filter(new FilterQuery(0, followArray, trackArray)); } private static boolean isNumericalArgument(String argument){ String args[] = argument.split(&quot;,&quot;); boolean isNumericalArgument = true; for(String arg : args){ try{ Integer.parseInt(arg); } catch(NumberFormatException nfe){ isNumericalArgument = false; break; } } return isNumericalArgument; }} Set up elasticsearch environment Go to AWS console and select elasticsearch service(you can also choose to use elasticsearch by downloading and unzipping it from here) Set access police to open access or any way you like, and then use curl to create index using index API.12345678$ curl -XPUT 'http://search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/' -d '{ &quot;settings&quot; : { &quot;index&quot; : { &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2 } }}' this piece of code will create an index named “user” in your AWS domain. Next, you are going to create mapping. Here, the mapping called “profile” is created, which has three properties: text, latitude and longitude.123456789101112curl -XPOST search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/_mapping/profile -d '{ &quot;profile&quot; : { &quot;properties&quot; : { &quot;text&quot; : { &quot;type&quot; : &quot;string&quot;}, &quot;latitude&quot; : { &quot;type&quot; : &quot;string&quot;}, &quot;longitude&quot; : { &quot;type&quot; : &quot;string&quot;} } }}' Upload data to elasticsearchTo upload data to elasticsearch, you can use the curl command in cygwin, since I am using a windows machine, or put this piece of code into the java program that are fetching twitter. From twitter stream, we can obtain a JSONObject, which could by utilize into uploading to elasticsearch. 12345678910111213141516String url = &quot;http://search-twitter-1-kf5qeriqw5iu6uasbyv6dmwfbq.us-west-2.es.amazonaws.com/user/profile&quot;;HttpClient client = HttpClientBuilder.create().build();HttpPost put = new HttpPost(url);put.setHeader(&quot;Content-type&quot;, &quot;application/json&quot;);StringEntity params =new StringEntity(json.toString());put.setEntity(params);HttpResponse response = client.execute(put);System.out.println(&quot;Response Code:&quot;+response.getStatusLine().getStatusCode()); BufferedReader rd = new BufferedReader(new InputStreamReader(response.getEntity().getContent()));StringBuffer result = new StringBuffer();String line = &quot;&quot;;while ((line = rd.readLine()) != null) { result.append(line);} System.out.println(&quot;result:&quot;+result); UI for our applicationThis is the entry point for our application, you can choose keyword for twitter.After you choose the keyword and click the “show” button, you can see the corresponding twitter location marked on the map.Every 5 seconds, the front end will refresh and the counter, which can be seen in the second picture as 10, will gradually increase as our backend program continuously fetching data from twitter.","link":"/2018/05/22/Using%20react%20component%20in%20twig/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2023/02/11/hello-world/"},{"title":"","text":"2018-12-24WIP: C++ 11(move, forward) Boost Future Project: Using boost in a docker instead of installing it 2018-12-26WIP: Spring Config Server SCons Docker Future Project: Use docker to deploy spring config server 2018-12-28WIP: Fig Fluentd Future Project: Use zipkins in distributed system","link":"/2023/02/12/Work%20Plan%20and%20Daily%20Reflection/"}],"tags":[{"name":"React","slug":"React","link":"/tags/React/"},{"name":"System Design","slug":"System-Design","link":"/tags/System-Design/"},{"name":"Operating System","slug":"Operating-System","link":"/tags/Operating-System/"},{"name":"Distributed System","slug":"Distributed-System","link":"/tags/Distributed-System/"},{"name":"Robotics","slug":"Robotics","link":"/tags/Robotics/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Intellisense","slug":"Intellisense","link":"/tags/Intellisense/"},{"name":"Network","slug":"Network","link":"/tags/Network/"},{"name":"Javascript","slug":"Javascript","link":"/tags/Javascript/"},{"name":"Cloud Computing","slug":"Cloud-Computing","link":"/tags/Cloud-Computing/"},{"name":"Maths","slug":"Maths","link":"/tags/Maths/"},{"name":"Lazy Loading","slug":"Lazy-Loading","link":"/tags/Lazy-Loading/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"Others","slug":"Others","link":"/tags/Others/"},{"name":"OOP","slug":"OOP","link":"/tags/OOP/"},{"name":"Regex","slug":"Regex","link":"/tags/Regex/"},{"name":"Mathematics","slug":"Mathematics","link":"/tags/Mathematics/"}],"categories":[{"name":"Frontend","slug":"Frontend","link":"/categories/Frontend/"},{"name":"System Design","slug":"System-Design","link":"/categories/System-Design/"},{"name":"Operating System","slug":"Operating-System","link":"/categories/Operating-System/"},{"name":"Distributed System","slug":"Distributed-System","link":"/categories/Distributed-System/"},{"name":"Robotics","slug":"Robotics","link":"/categories/Robotics/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"Intellisense","slug":"Intellisense","link":"/categories/Intellisense/"},{"name":"Network","slug":"Network","link":"/categories/Network/"},{"name":"Programming Language","slug":"Programming-Language","link":"/categories/Programming-Language/"},{"name":"Cloud Computing","slug":"Cloud-Computing","link":"/categories/Cloud-Computing/"},{"name":"Maths","slug":"Maths","link":"/categories/Maths/"},{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"Others","slug":"Others","link":"/categories/Others/"},{"name":"Regex","slug":"Regex","link":"/categories/Regex/"},{"name":"Mathemtatics","slug":"Mathemtatics","link":"/categories/Mathemtatics/"}],"pages":[]}